% Neural network training and performance assessment
%
% This script roughly follows the workflow in
% https://de.mathworks.com/help/stats/assess-regression-neural-network-performance.html
% to train a regression neural network friction model and assess its
% performance on a test data set.
%
% Requires: data sets generated by V_generate_friction_data.m
%
% Author: Kerstin Vater, MSc
% Machine Learning Dynamics Group (M-14)
% Hamburg University of Technology
% Am Schwarzenberg-Campus 1
% 21073 Hamburg, Germany
% E-mail: kerstin.vater@tuhh.de  
% URL: https://www.tuhh.de/dyn

%------------- BEGIN CODE --------------

% clear
% close all

% Load custom colormaps
load("colors\viridis.mat")
load("colors\Set1.mat")
set(0, "DefaultAxesColorOrder", Set1)

% Load friction data sets
dataFull  = readtable("exponential_friction_model_samples_full.csv");
dataTrain = readtable("exponential_friction_model_samples_training.csv");
dataTest  = readtable("exponential_friction_model_samples_test.csv");

% TODO Load optimized neural network model parameters
% optimizedModelParams = load("optimized_nn_model_parameters.mat");

%% Train regression neural network
% Train an optimized regression neural network model using the training
% data set and the test data set for validation purposes

rng("default") % Restore the random number generator seed for reproducibility

rnet = fitrnet(dataTrain, "Ff", ...
    "ValidationData", dataTest, ...
    "Activations",  "tanh", ...
    "Lambda",       0.004, ...
    "LayerSizes",   [100 100 100], ...
    "Standardize",  true, ...
    "StoreHistory", true, ...
    "Verbose",      true);

fprintf("\nTraining completed.\n\n");

% Set up figure
figure
sgtitle("Neural network training and performance")
set(gcf, "WindowState", "maximized")

% Plot the training mean squared error (MSE) and the validation MSE at each iteration
iteration = rnet.TrainingHistory.Iteration;
trainLosses = rnet.TrainingHistory.TrainingLoss;
valLosses = rnet.TrainingHistory.ValidationLoss;
subplot(2,2,1)
semilogy(iteration, trainLosses, iteration, valLosses)
legend(["Training", "Test"])
title("Training History")
xlabel("Iteration")
ylabel("Mean squared error (loss)")
grid on

%% Evaluate test set performance

% Compute the test set mean squared error (MSE)
testMSE = loss(rnet, dataTest, "Ff");
fprintf('Mean squared error (MSE) on test data = %6.4f.\n', testMSE);

% Compare the predicted test set response values to the true response values
% Note: a good model produces predictions that are scattered near the line
subplot(2,2,2)
testPredictions = predict(rnet, dataTest(:,1:2));
plot(dataTest.Ff, testPredictions, ".")
hold on
plot(dataTest.Ff, dataTest.Ff)
title("Predicted Test Set Response")
xlabel("True friction force")
ylabel("Predicted friction force")
legend("Predicted", "True", "Location", "southeast")
grid on

% Plot the test set residuals
% Note: a good model usually has residuals scattered roughly symmetrically
% around 0
subplot(2,2,3)
residuals = dataTest.Ff - testPredictions;
plot(dataTest.Ff, residuals, ".")
hold on
plot(dataTest.Ff, zeros(size(dataTest.Ff)))
title("Test Set Residuals")
xlabel("True friction force")
ylabel("Friction force residuals")
legend("Predicted", "True", "Location", "southeast")
grid on

% Compute the coefficient of determination (R-squared)
% Note: a good model yields an R-squared value close to 1
testR2 = 1 - sum(residuals.^2) / sum((dataTest.Ff - mean(dataTest.Ff)).^2);
fprintf('Coefficient of determination (R-squared) on test data = %6.4f.\n', testR2);

% Plot empirical Cumulative Distribution Function (eCDF)
subplot(2,2,4)
cdfplot(testPredictions);
hold on;
cdfplot(dataTrain.Ff);
ylabel('Empirical cumulative probability');
xlabel("Friction force");
legend("Predicted", "True", "Location", "southeast")


%% Sample and plot predicted friction force

fmodel.type = 'exponential';
fmodel.a    = 5.0;      % Control parameter for the negative gradient of the friction curve
fmodel.mus  = 0.5;      % Static coefficient of friction                   [-]
fmodel.muk  = 0.2;      % Kinetic coefficient of friction                  [-]
Fn          = 100.0;    % Normal force                                     [N]
eps         = 1e-4;     % Threshold value for relative sliding velocity    [m/s]

% Compute maximum static friction force [N]
Fcrit = Fn * fmodel.mus;

% Sample predicted friction coefficient and friction force
vs = linspace(-max(abs(dataFull.vs)), max(abs(dataFull.vs)), 1e5)';
Xsample = table(Fn.*ones(size(vs)), vs, 'VariableNames', {'Fn','vs'});
Ff = predict(rnet, Xsample);
mu = abs(Ff ./ Fn);

% Sample analytical friction coefficient and friction force
[FfRef, muRef] = getFrictionForce(Fn, vs, fmodel);

% Set up figure
figure
sgtitle("Regression neural network friction models")
set(gcf, "WindowState", "maximized")

% Plot Stribeck curve
subplot(2,2,1)
plot(vs, mu)
hold on
plot(vs, muRef)
plot(vs, +fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
plot(vs, -fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
title('Coefficient of Friction')
xlabel('v_{s} [m/s]')
ylabel('\mu [-]')
xlim([0.0, 0.1]) % xlim([0.0, max(vs)])
ylim([0.0, 1.2*fmodel.mus])
grid on

yyaxis right
plot(vs, +fmodel.muk*ones(size(vs)), '--')
plot(vs, -fmodel.muk*ones(size(vs)), '--')
plot(vs, +fmodel.mus*ones(size(vs)), '--')
plot(vs, -fmodel.mus*ones(size(vs)), '--')
ylim([0.0, 1.2*fmodel.mus])
yticks([-fmodel.mus, -fmodel.muk, fmodel.muk, fmodel.mus])
yticklabels({'-\mu_s', '-\mu_k', '\mu_k', '\mu_s'})
legend("Neural network", "Analytical", "Location", "southeast")

% Plot friction force
subplot(2,2,3)
plot(vs, Ff)
hold on
plot(vs, FfRef)
plot(vs, +Fn*fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
plot(vs, -Fn*fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
title('Friction Force')
xlabel('v_{s} [m/s]')
ylabel('F_{f} [N]')
xlim([-0.07, 0.07]) % xlim([min(vs), max(vs)])
ylim([-1.2*Fcrit, 1.2*Fcrit])
grid on

yyaxis right
plot(vs, +Fn*fmodel.muk*ones(size(vs)), '--')
plot(vs, -Fn*fmodel.muk*ones(size(vs)), '--')
plot(vs, +Fn*fmodel.mus*ones(size(vs)), '--')
plot(vs, -Fn*fmodel.mus*ones(size(vs)), '--')
ylim([-1.2*Fcrit, 1.2*Fcrit])
yticks([-Fn*fmodel.mus, -Fn*fmodel.muk, Fn*fmodel.muk, Fn*fmodel.mus])
yticklabels({'-F_n \mu_s', '-F_n \mu_k', 'F_n \mu_k', 'F_n \mu_s'})
legend("Neural network", "Analytical", "Location", "southeast")

subplot(2,2,4)
plot(vs, Ff)
hold on
plot(vs, FfRef)
plot(vs, +Fn*fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
plot(vs, -Fn*fmodel.muk*ones(size(vs)), '--', 'Color', Set1(end,:))
title('Friction Force (Closeup)')
xlabel('v_{s} [m/s]')
ylabel('F_{f} [N]')
xlim([-5e-3, 5e-3])
ylim([-1.2*Fcrit, 1.2*Fcrit])
grid on

yyaxis right
plot(vs, +Fn*fmodel.muk*ones(size(vs)), '--')
plot(vs, -Fn*fmodel.muk*ones(size(vs)), '--')
plot(vs, +Fn*fmodel.mus*ones(size(vs)), '--')
plot(vs, -Fn*fmodel.mus*ones(size(vs)), '--')
ylim([-1.2*Fcrit, 1.2*Fcrit])
yticks([-Fn*fmodel.mus, -Fn*fmodel.muk, Fn*fmodel.muk, Fn*fmodel.mus])
yticklabels({'-F_n \mu_s', '-F_n \mu_k', 'F_n \mu_k', 'F_n \mu_s'})
legend("Neural network", "Analytical", "Location", "southeast")

%% Perform 5-fold cross-validation

rng("default") % Restore the random number generator seed for reproducibility

% Train an optimized regression neural network model using the full data set
cvrnet = fitrnet(dataFull, "Ff", ...
    "Activations",  "tanh", ...
    "Lambda",       0.004, ...
    "LayerSizes",   [100 100 100], ...
    "Standardize",  true);

partitionedModel = crossval(cvrnet, 'KFold', 5);

% Compute regression loss for observations not used for training
kfLoss = kfoldLoss(partitionedModel, "Mode", "individual", "LossFun", "mse");

% Compute max value, mean value and standard deviation of loss values
fprintf("\n5-fold cross-validation completed.\n")
fprintf("Maximum loss = %6.4f\n", max(kfLoss));
fprintf("Mean loss = %6.4f\n", mean(kfLoss));
fprintf("Standard deviation of loss values = %6.4f\n", std(kfLoss));
fprintf("Validation RMSE = %6.4f\n", sqrt(mean(kfLoss)));


%% Export final regression neural network friction model
save("trained_regression_nn_friction_model.mat", "rnet");
fprintf("\nSaved fit neural network model to workspace.\n");

%------------- END OF CODE --------------